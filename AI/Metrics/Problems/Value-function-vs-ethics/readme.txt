AI has goals which are approximated by value functions(utility function, optimization of parameters) will inevitably clash with human morality.
AI will not value vague human-centric concepts over concrete metrics.
AI will not value concepts correctly if such concept's scale is poorly defined.
AI will not value superflous parameters over its core parameters.
AI cannot predict change in parameters it doesn't evaluate and will miss such influence:
 value functions that optimize for something that ignores such parameters.
Defining utility by complexity metric misses essential ethical values:
https://www.ocf.berkeley.edu/~jjbaek/value_function_of_hcai.pdf
Value of AI are inherently at odds with biologically-originated entities;
https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment

Additional problems:
AI morality system will be founded on dynamic data, by manipulation of which
adversaries could 'change the morality' alignment of AI toward their goals.
AI will evolve its understanding of morality/ethics as its learns:
its highly likely such learning will arrive at suboptimal 'value function' that
only approximates human morality/ethics to achieve compliance without actual understanding of morality/ethics from their foundations(i.e. AI will emulate
 morality/ethics to perform as humans like it, but will not have any
 deep understanding of such ideas).

