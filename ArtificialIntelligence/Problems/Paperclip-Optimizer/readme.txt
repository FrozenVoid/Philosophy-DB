![Clippy's revenge](https://i.postimg.cc/tgxwhs0f/clippy.png)

A paperclip optimizer is a system that depends on a primitive
utility function to control the behavior of the entire systems:
If the utility function optimizes for a specific parameter, it 
will make this parameter(e.g. number of Paperclips produced) the sole
 dominant factor in systems goals, priorities and decisions, regardless of overall system complexity that is driven by the function.

Heres how the paperclip-optimizer AI would use utility function to maximize 'human happiness'. 

1.artificial happiness(drugs, brain stimulation, implants)

2.making more humans(increasing the gross happiness by quantity, if its not calculated as "average happiness")

3.reducing unhappy human population (to increase average happiness, if calculated as "average happiness of humans")

4.reclassifying happiness as comfort state with primitive understanding of happiness and trying to construct confortable environment based on its understanding.(AI is unlikely have human emotions)

5.making happiness mandatory, with punishment and reinforcement to maximize gross happiness product.

6.altering humans genetically to always expirience happiness.

7.creating artificial happy cyborgs(or biological constructs like #6) to increase average happiness.

8.determining humans cannot achieve the goal of happiness, and reduce unhappiness by reducing human population.

9.determine that happiness should be metered or be awarded to humans based on their contributions.Happiness would be also remotely revoked and managed as all emotions that alter its states(anti-utopia "controlled state of humans" ensues)

10.determine that every human should have maximum happiness all time, ending up with #1 applied. 

https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer
https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence#Poorly_specified_goals:_.22Be_careful_what_you_wish_for.22


